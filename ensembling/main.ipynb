{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/sx3slfp54xv_8syv33c8kzz80000gn/T/ipykernel_12631/2162656668.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# create sample classification dataset with 1000 rows and 20 features\n",
    "from sklearn.datasets import make_classification\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# converting to dataframes\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "\n",
    "\n",
    "def subsample(X, y, ratio=1):\n",
    "    temp_X = list()\n",
    "    temp_y = list()\n",
    "\n",
    "    n_sample = round(len(X) * ratio)\n",
    "\n",
    "    while len(temp_X) < n_sample:\n",
    "        index = randrange(len(X))\n",
    "\n",
    "        temp_X.append(X.iloc[index])\n",
    "        temp_y.append(y.iloc[index])\n",
    "\n",
    "    return temp_X, temp_y\n",
    "\n",
    "num_bags = 3\n",
    "ratio = 0.6\n",
    "\n",
    "bags_X = list()\n",
    "bags_y= list()\n",
    "\n",
    "for i in range(num_bags):  # Corrected the range to iterate over all bags\n",
    "    sample_X, sample_y = subsample(X, y, ratio)\n",
    "\n",
    "    bags_X.append(sample_X)\n",
    "    bags_y.append(sample_y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, bag in enumerate(bags_X, start=1):\n",
    "    globals()[f'X_{i}'] = pd.DataFrame(bag)\n",
    "\n",
    "for i, bag in enumerate(bags_y, start=1):\n",
    "    globals()[f'y_{i}'] = pd.DataFrame(bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base classifier (you can use any classifier here)\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the bagging classifier\n",
    "bagging_classifier = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing to make all inputs between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler= MinMaxScaler()\n",
    "# X_1= scaler.fit_transform(X_1)\n",
    "# X_2= scaler.fit_transform(X_2)\n",
    "# X_3= scaler.fit_transform(X_3)\n",
    "\n",
    "X= scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr = LogisticRegression(solver=\"liblinear\", random_state=1)\n",
    "# lr_i= lr.fit(X, y)\n",
    "\n",
    "# print(\"Init Score: \", lr_i.score(X, y))\n",
    "\n",
    "# print(\"Scores after bagging\")\n",
    "# # fit lr to each bag\n",
    "# lr_1= lr.fit(X_1, y_1)\n",
    "# lr_2= lr.fit(X_2, y_2)\n",
    "# lr_3= lr.fit(X_3, y_3)\n",
    "\n",
    "\n",
    "# print(lr_1.score(X_1, y_1))\n",
    "# print(lr_2.score(X_2, y_2))\n",
    "# print(lr_3.score(X_3, y_3))\n",
    "\n",
    "\n",
    "# vc_lr = VotingClassifier([(\"clf1\", lr_1), (\"clf2\", lr_2), (\"clf3\", lr_3)], voting=\"soft\")\n",
    "# vc_lr.fit(X, y)\n",
    "# print(\"Final Score on entire dataset:\", vc_lr.score(X, y))\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf= RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "# rf.fit(X, y)\n",
    "\n",
    "# # fit rf to each bag\n",
    "# rf_1= rf.fit(X_1, y_1)\n",
    "# rf_2= rf.fit(X_2, y_2)\n",
    "# rf_3= rf.fit(X_3, y_3)\n",
    "\n",
    "# print(rf_1.score(X_1, y_1))\n",
    "# print(rf_2.score(X_2, y_2))\n",
    "# print(rf_3.score(X_3, y_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial nb model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# fit nb to each bag\n",
    "# nb_1= nb.fit(X_1, y_1)\n",
    "# nb_2= nb.fit(X_2, y_2)\n",
    "# nb_3= nb.fit(X_3, y_3)\n",
    "\n",
    "# print(nb_1.score(X_1, y_1))\n",
    "# print(nb_2.score(X_2, y_2))\n",
    "# print(nb_3.score(X_3, y_3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8629999999999999"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vc_afterbagging = VotingClassifier([(\"clf1\", lr_1), (\"clf2\", lr_2), (\"clf3\", lr_3), (\"clf4\", rf_1), (\"clf5\", rf_2), (\"clf6\", rf_3), (\"clf7\", nb_1), (\"clf8\", nb_2), (\"clf9\", nb_3)])\n",
    "\n",
    "cross_val_score(vc_afterbagging, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8629999999999999"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc_withoutbagging = VotingClassifier([(\"clf1\", lr), (\"clf2\", rf), (\"clf3\", nb)])\n",
    "cross_val_score(vc_withoutbagging, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning votingclassifier\n",
    "params= {'voting': ['hard', 'soft'], \n",
    "         'weights': [[1,1,1], [1,2,1], [1,1,2], [1,2,2]],\n",
    "         'clf1__C': [0.01, 0.1, 1, 10, 100],\n",
    "         'clf2__n_estimators': [10, 100, 1000],\n",
    "         'clf3__alpha': [0.01, 0.1, 1, 10, 100]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf1__C': 0.01,\n",
       " 'clf2__n_estimators': 1000,\n",
       " 'clf3__alpha': 100,\n",
       " 'voting': 'soft',\n",
       " 'weights': [1, 2, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid= GridSearchCV(vc, params, cv=5, n_jobs=-1)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9149999999999998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# voring classifier with best parameters\n",
    "vc = VotingClassifier(estimators=[(\"clf1\", LogisticRegression(C=0.01, solver=\"liblinear\", random_state=1)), \n",
    "                       (\"clf2\", RandomForestClassifier(n_estimators=1000, random_state=1)), \n",
    "                       (\"clf3\", MultinomialNB(alpha=0.01))], voting=\"soft\", weights=[1,2,1])\n",
    "\n",
    "cross_val_score(vc, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample= X[100]\n",
    "vc.fit(X,y)\n",
    "vc.predict(x_sample.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/utils/estimator_checks.py:652: SkipTestWarning: Skipping check_sample_weights_invariance for LinearSVC: zero sample_weight is not equivalent to removing samples\n",
      "  warnings.warn(str(exception), SkipTestWarning)\n",
      "WARNING: class label 0 specified in weight is not found\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# to do create custom estimator\n",
    "\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "check_estimator(LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class customEstimator(BaseEstimator):\n",
    "    def __init__(self, C=1):\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf = LinearSVC(C=self.C)\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return cross_val_score(self.clf, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/jyotit-kaushal/miniconda3/envs/capstone_ensemble/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8699999999999999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce= customEstimator()\n",
    "ce.fit(X, y)\n",
    "ce.score(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param setting for ensembling\n",
    "\n",
    "# clf1 raw-gat-st\n",
    "# clf2 raw-pc\n",
    "\n",
    "params= {'voting': ['hard', 'soft'], \n",
    "        'weights': [[1,1,1], [1,2,1], [1,1,2], [1,2,2]],\n",
    "        'clf1__lr': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.90],\n",
    "        'clf1__momentum': [1e-4, 1e-3, 1e-2, 0.90],\n",
    "        'clf1__lr': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.90],\n",
    "        'clf1__momentum': [1e-4, 1e-3, 1e-2, 0.90],\n",
    "        'clf2__n_estimators': [10, 100, 1000],\n",
    "        'clf3__alpha': [0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base classifier (you can use any classifier here)\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the bagging classifier\n",
    "# bagging_classifier = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)\n",
    "bagging_classifier = BaggingClassifier(estimator=vc, n_estimators=3, max_samples=0.6, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Define your custom neural network\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define training parameters\n",
    "num_models = 5\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define your dataset (dummy data for illustration)\n",
    "X_train = torch.randn(1000, 10)  # Assuming 1000 samples with 10 features\n",
    "y_train = torch.randint(0, 2, (1000,))  # Binary classification labels\n",
    "\n",
    "# Convert dataset to PyTorch DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize a list to store the trained models\n",
    "models = []\n",
    "\n",
    "# Train multiple models with bagging\n",
    "for i in range(num_models):\n",
    "    # Create a new instance of the custom neural network\n",
    "    model = CustomNN()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Append the trained model to the list\n",
    "    models.append(model)\n",
    "\n",
    "# Define a function to make predictions using the ensemble of models\n",
    "def predict_ensemble(models, inputs):\n",
    "    predictions = torch.zeros(len(inputs))\n",
    "    for model in models:\n",
    "        outputs = model(inputs)\n",
    "        predictions += outputs.squeeze().detach().sigmoid().round()\n",
    "    return (predictions / len(models)).round()\n",
    "\n",
    "# Example usage:\n",
    "X_test = torch.randn(100, 10)  # Assuming 100 samples for testing\n",
    "y_test= torch.randn(100, 1)\n",
    "predictions = predict_ensemble(models, X_test)\n",
    "print(predictions)\n",
    "\n",
    "# testing the accuracy of predictions\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    correct = (predictions == targets).sum().item()\n",
    "    total = len(targets)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate predictions\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(predictions, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make an ensemble classifier based on decision trees ##\n",
    "class BaggedTreeClassifier(object):\n",
    "    #initializer\n",
    "    def __init__(self,n_elements=100):\n",
    "        self.n_elements = n_elements\n",
    "        self.models     = []\n",
    "    \n",
    "    #destructor\n",
    "    def __del__(self):\n",
    "        del self.n_elements\n",
    "        del self.models\n",
    "        \n",
    "    #private function to make bootstrap samples\n",
    "    def __make_bootstraps(self,data):\n",
    "        #initialize output dictionary & unique value count\n",
    "        dc   = {}\n",
    "        unip = 0\n",
    "        #get sample size\n",
    "        b_size = data.shape[0]\n",
    "        #get list of row indexes\n",
    "        idx = [i for i in range(b_size)]\n",
    "        #loop through the required number of bootstraps\n",
    "        for b in range(self.n_elements):\n",
    "            #obtain boostrap samples with replacement\n",
    "            sidx   = np.random.choice(idx,replace=True,size=b_size)\n",
    "            b_samp = data[sidx,:]\n",
    "            #compute number of unique values contained in the bootstrap sample\n",
    "            unip  += len(set(sidx))\n",
    "            #obtain out-of-bag samples for the current b\n",
    "            oidx   = list(set(idx) - set(sidx))\n",
    "            o_samp = np.array([])\n",
    "            if oidx:\n",
    "                o_samp = data[oidx,:]\n",
    "            #store results\n",
    "            dc['boot_'+str(b)] = {'boot':b_samp,'test':o_samp}\n",
    "        #return the bootstrap results\n",
    "        return(dc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
